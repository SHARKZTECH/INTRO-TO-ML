{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGSJMhy/lo96rEL3xq4odu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SHARKZTECH/INTRO-TO-ML/blob/main/Fraud_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2fy8rmFfIx9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeUww_1IJ0jo",
        "outputId": "d77e30c2-a8f4-4738-a795-761d94c55aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Importing the Dataset\n",
        "# Upload the dataset file to your Colab environment\n",
        "# For example, if the dataset is named \"credit_card_dataset.csv\":\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/creditcard.csv\")\n"
      ],
      "metadata": {
        "id": "t04thFj-fduU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data Preprocessing\n",
        "# Count the number of empty columns\n",
        "empty_columns = data.columns[data.isnull().any()].tolist()\n",
        "print(\"Number of empty columns:\", len(empty_columns))\n",
        "\n",
        "# Drop columns with empty values\n",
        "data = data.dropna(axis=1, how='any')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvpNh83vUjA4",
        "outputId": "da096f72-ec07-4270-88f1-7f5165719abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of empty columns: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assuming the dataset has been preprocessed and split into features (X) and labels (y)\n",
        "X = data.drop(\"Class\", axis=1)\n",
        "y = data[\"Class\"]\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
        "\n",
        "# Apply standardization to numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "L3pE7nEPfW5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 4: Model Selection\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Step 5: Model Training\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Model Evaluation\n",
        "# Evaluate the model on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Step 7: Testing\n",
        "# Evaluate the model on the testing set\n",
        "y_test_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YojOjxWfRKy",
        "outputId": "b7ffe4dd-4edc-446a-fec7-85c77a91b564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     42644\n",
            "           1       0.86      0.57      0.69        77\n",
            "\n",
            "    accuracy                           1.00     42721\n",
            "   macro avg       0.93      0.79      0.84     42721\n",
            "weighted avg       1.00      1.00      1.00     42721\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     42663\n",
            "           1       0.89      0.71      0.79        59\n",
            "\n",
            "    accuracy                           1.00     42722\n",
            "   macro avg       0.95      0.86      0.90     42722\n",
            "weighted avg       1.00      1.00      1.00     42722\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing diff algo\n",
        "In this code, three different algorithms are selected and trained: Logistic Regression, Random Forest, and Support Vector Machines (SVC). Each model is trained and evaluated using the validation set. The evaluation metrics include accuracy, precision, recall, and F1-score.\n",
        "\n",
        "After evaluating all the models, you can select the best model based on the validation performance. The code assumes you have a mechanism in place for selecting the best model (e.g., based on the highest F1-score). Finally, the best model is evaluated on the testing set, and the evaluation metrics are printed."
      ],
      "metadata": {
        "id": "stmgRHC2VQ5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Importing the Dataset\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/creditcard.csv\")\n",
        "\n",
        "# Step 3: Data Preprocessing\n",
        "\n",
        "# ... (Code for data preprocessing)\n",
        "\n",
        "# Assuming the dataset has been preprocessed and split into features (X) and labels (y)\n",
        "X = data.drop(\"Class\", axis=1)\n",
        "y = data[\"Class\"]\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
        "\n",
        "# Apply standardization to numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Model Selection and Training\n",
        "models = [\n",
        "    RandomForestClassifier(),\n",
        "    LogisticRegression(),\n",
        "    SVC()\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Step 6: Model Evaluation\n",
        "    # Evaluate the model on the validation set\n",
        "    y_val_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    precision = precision_score(y_val, y_val_pred)\n",
        "    recall = recall_score(y_val, y_val_pred)\n",
        "    f1 = f1_score(y_val, y_val_pred)\n",
        "\n",
        "    # Print the evaluation metrics\n",
        "    print(\"Model:\", model.__class__.__name__)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-score:\", f1)\n",
        "    print()\n",
        "\n",
        "# Step 7: Testing\n",
        "# Step 7: Selecting the Best Model\n",
        "best_model = None\n",
        "best_f1 = 0.0\n",
        "\n",
        "for model in models:\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    f1 = f1_score(y_val, y_val_pred)\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_model = model\n",
        "\n",
        "# Step 8: Testing\n",
        "# Evaluate the best model on the testing set\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics on the testing set\n",
        "accuracy = accuracy_score(y_test, y_test_pred)\n",
        "precision = precision_score(y_test, y_test_pred)\n",
        "recall = recall_score(y_test, y_test_pred)\n",
        "f1 = f1_score(y_test, y_test_pred)\n",
        "\n",
        "# Print the evaluation metrics on the testing set\n",
        "print(\"Best Model (Testing Set):\", best_model.__class__.__name__)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "\n"
      ],
      "metadata": {
        "id": "aoJmkD6sfWPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c255e23-9ecc-43fe-f8ab-478b7e80ba55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model: RandomForestClassifier\n",
            "Accuracy: 0.9995084384728822\n",
            "Precision: 0.9375\n",
            "Recall: 0.7792207792207793\n",
            "F1-score: 0.851063829787234\n",
            "\n",
            "Model: LogisticRegression\n",
            "Accuracy: 0.9990636923292994\n",
            "Precision: 0.8627450980392157\n",
            "Recall: 0.5714285714285714\n",
            "F1-score: 0.6875\n",
            "\n",
            "Model: SVC\n",
            "Accuracy: 0.9992041384799045\n",
            "Precision: 0.9387755102040817\n",
            "Recall: 0.5974025974025974\n",
            "F1-score: 0.73015873015873\n",
            "\n",
            "Best Model (Testing Set): RandomForestClassifier\n",
            "Accuracy: 0.9997425214175366\n",
            "Precision: 0.9444444444444444\n",
            "Recall: 0.864406779661017\n",
            "F1-score: 0.9026548672566371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming 'best_model' is the trained RandomForestClassifier model\n",
        "\n",
        "# Save the model to a file\n",
        "filename = 'fraud_detection_model.pkl'\n",
        "with open(filename, 'wb') as file:\n",
        "    pickle.dump(best_model, file)\n"
      ],
      "metadata": {
        "id": "_Qh9lFNjasuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model from file\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/fraud_detection_model.pkl\", 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n"
      ],
      "metadata": {
        "id": "NzvjTEH5atud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5  # Adjust the threshold based on your requirements\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Sample transaction features\n",
        "sample_transaction = np.array([0.5, -0.3, 1.2, 0.8, -0.5, 0.2, -0.1, 0.3, 0.2, 0.4, -0.5, 0.7, -0.2, 0.1, 0.2, -0.4, 0.6, 0.2, 0.3, -0.1, 100.0])\n",
        "\n",
        "# Add dummy values for the missing features\n",
        "missing_features = 30 - len(sample_transaction)\n",
        "dummy_values = np.zeros(missing_features)\n",
        "sample_transaction = np.concatenate((sample_transaction, dummy_values))\n",
        "\n",
        "# Reshape the transaction features to match the model's input shape\n",
        "sample_transaction = sample_transaction.reshape(1, -1)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "prediction_probabilities = loaded_model.predict_proba(sample_transaction)\n",
        "\n",
        "# Compare the prediction probabilities with the threshold\n",
        "if prediction_probabilities[0][1] >= threshold:\n",
        "    # Transaction classified as potentially fraudulent\n",
        "    print(\"Potential Fraudulent Transaction\")\n",
        "else:\n",
        "    # Transaction classified as legitimate\n",
        "    print(\"Legitimate Transaction\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD4C6TtkayHQ",
        "outputId": "d45c20bc-bcbf-44f0-f46c-90f3b95bf403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Legitimate Transaction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJPl98UTiQyk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}